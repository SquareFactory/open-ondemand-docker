##-- Cluster definition
ClusterName=reindeerpizza

SlurmUser=slurm

#-- Slurmctl
SlurmctldHost=slurm-cluster-reindeer-controller-0
SlurmctldDebug=debug5
SlurmctldParameters=enable_configless
StateSaveLocation=/var/spool/slurmctl
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldLogFile=/var/log/slurm/slurmctld.log
PrologSlurmctld=/etc/slurm/prolog-slurmctld
EpilogSlurmctld=/etc/slurm/epilog-slurmctld

#-- Slurmd
SlurmdDebug=debug5
SlurmdLogFile=/var/log/slurm/slurmd.log
SrunPortRange=60001-63000

#-- Default ressources allocation
DefCpuPerGPU=4
DefMemPerCpu=7000


#-- Scheduling
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_CPU_Memory
SchedulerTimeSlice=60
UnkillableStepTimeout=300


#-- Multifactor priority
PriorityType=priority/multifactor
# The larger the job, the greater its job size priority.
PriorityFavorSmall=NO
# The job's age factor reaches 1.0 after waiting in the
# queue for 2 weeks.
#PriorityMaxAge=14-0
# This next group determines the weighting of each of the
# components of the Multi-factor Job Priority Plugin.
# The default value for each of the following is 1.
PriorityWeightAge=0
PriorityWeightFairshare=0
PriorityWeightJobSize=0
PriorityWeightPartition=0
PriorityWeightQOS=100
PriorityDecayHalfLife=0
PriorityUsageResetPeriod=MONTHLY


#-- Accounting
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost=slurmdbd.csquare.gcloud
AccountingStoragePort=6819
AccountingStorageTRES=gres/gpu


#-- Multi Authentication
AuthType=auth/munge
AuthAltTypes=auth/jwt
AuthAltParameters=jwt_key=/var/spool/slurm/jwt_hs256.key


#-- Compute nodes
NodeName=cn[1-11]  CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4
NodeName=cn12 CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128473 Gres=gpu:4


#-- Partitions
PartitionName=main Nodes=cn[1-11] Default=YES MaxTime=INFINITE State=UP OverSubscribe=NO TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
PartitionName=main-beeond Nodes=cn[1-11] Default=NO MaxTime=INFINITE State=UP OverSubscribe=EXCLUSIVE TRESBillingWeights="CPU=2.6,Mem=0.25G,GRES/gpu=24.0"
PartitionName=private Nodes=cn12 Default=NO MaxTime=INFINITE State=UP OverSubscribe=NO Hidden=YES AllowQos=admin


#-- Extra
LaunchParameters=enable_nss_slurm
DebugFlags=Script,Gang,SelectType
TCPTimeout=5

# MPI stacks running over Infiniband or OmniPath require the ability to allocate more
# locked memory than the default limit. Unfortunately, user processes on login nodes
# may have a small memory limit (check it by ulimit -a) which by default are propagated
# into Slurm jobs and hence cause fabric errors for MPI.
PropagateResourceLimitsExcept=MEMLOCK

ProctrackType=proctrack/cgroup
TaskPlugin=task/cgroup
SwitchType=switch/none
MpiDefault=pmix_v2
ReturnToService=2 #temp
GresTypes=gpu
PreemptType=preempt/qos
PreemptMode=REQUEUE
PreemptExemptTime=-1
Prolog=/etc/slurm/prolog.d/*
Epilog=/etc/slurm/epilog.d/*

# Federation
FederationParameters=fed_display

JobCompType=jobcomp/report
JobCompLoc=https://slurm:f_4UyDHC7%21w%24Cc2KSHFv@slurm.accounting.csquare.gcloud/report
JobAcctGatherType=jobacct_gather/cgroup
MailProg=/usr/bin/fakemail
